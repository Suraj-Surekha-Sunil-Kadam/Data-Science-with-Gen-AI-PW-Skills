{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "9bxKPA1_qh2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks that splits data into subsets based on feature values, forming a tree-like structure of decisions which ultimately lead to predicted outcomes or values.\n",
        "\n",
        "*   How Decision Trees Work in Classification\n",
        "\n",
        "    * The process begins at the root node, representing the entire dataset.\n",
        "\n",
        "    * At each decision (internal) node, data is split according to rules based on feature values (e.g., \"Age > 30?\", \"Income > $50,000?\").\n",
        "\n",
        "    * This splitting is usually determined by measures like Gini impurity or information gain, which help find the most discriminative feature for the split.\n",
        "\n",
        "    * Branches connect nodes and represent possible values or outcomes for each decision.\n",
        "\n",
        "    * The process continues recursively, forming further internal nodes and splits, until a stopping criterion is reached (such as all observations in a node are of the same class, or a maximum tree depth is met).\n",
        "\n",
        "    * Leaf nodes represent the final decision or predicted class for the given subset of data.\n",
        "\n",
        "    * For classification, the outcome in each leaf is generally a categorical value, like \"spam\" or \"not spam\".\n",
        "\n",
        "*   Example:\n",
        "\n",
        "    For classifying whether a customer will buy a product, a decision tree may ask questions at each node:\n",
        "\n",
        "    \"Income > $50,000?\" → If yes, go to the next question; if no, predict \"No Purchase\".\n",
        "\n",
        "    \"Age > 30?\" → If yes, go to next question; if no, predict \"No Purchase\".\n",
        "\n",
        "    \"Previous Purchases > 0?\" → If yes, predict \"Purchase\"; if no, predict \"No Purchase\".\n",
        "\n",
        "    Each path from the root to a leaf defines a decision rule mapping input features to a class label."
      ],
      "metadata": {
        "id": "SEE6Q_Lcqkwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Gini Impurity and Entropy are both impurity measures used in Decision Trees to evaluate how well a feature splits the data at each node, guiding the tree-building process to create the most homogeneous branches possible.\n",
        "\n",
        "*   Impurity Measures:\n",
        "\n",
        "    Gini Impurity and Entropy\n",
        "    \n",
        "    To decide how to split nodes, Decision Trees use impurity measures that quantify how mixed the classes are within a node. Two common measures:\n",
        "\n",
        "*   Gini Impurity:\n",
        "    \n",
        "    Measures the chance of misclassifying a randomly chosen sample from the node if it were labeled according to class distribution.\n",
        "\n",
        "    Formula:\n",
        "\n",
        "    Gini = 1 - sum(p_j^2 for each class j)\n",
        "    \n",
        "    Ranges 0 (pure node with one class) to max (most mixed).\n",
        "\n",
        "    Splits aim to minimize Gini impurity.\n",
        "\n",
        "*   Entropy:\n",
        "      \n",
        "    Measures the disorder or uncertainty in the node.\n",
        "\n",
        "    Formula:\n",
        "\n",
        "    Entropy = - sum(p_j * log2(p_j) for each class j)\n",
        "\n",
        "    0 means pure node, max when classes are evenly distributed.\n",
        "\n",
        "    The algorithm chooses splits that reduce entropy the most (maximize information gain).\n",
        "\n",
        "*   Impact on Decision Tree Splits:\n",
        "\n",
        "    * At each node, possible splits are evaluated by calculating the weighted average impurity (Gini or Entropy) of child nodes.\n",
        "\n",
        "    * The best split is the one that reduces impurity the most (lowest weighted average impurity).\n",
        "\n",
        "    * Gini is generally faster to compute and works well for binary splits.\n",
        "\n",
        "    * Entropy is theoretically grounded in information theory and may perform better for multi-class problems.\n",
        "\n",
        "    * Both generally lead to similar tree structures and accuracy."
      ],
      "metadata": {
        "id": "Gfw3NhwHqkz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Pre-Pruning and Post-Pruning are two techniques used to prevent overfitting in Decision Trees by controlling their size and complexity.\n",
        "\n",
        "*   Pre-Pruning (Early Stopping):\n",
        "\n",
        "    * Pre-Pruning stops the tree growth early during the training process before it becomes too complex.\n",
        "\n",
        "    * It uses criteria like maximum tree depth, minimum samples per leaf, or minimum information gain to halt further splitting.\n",
        "\n",
        "    * This method avoids building deep branches that have little benefit, thus preventing overfitting from the start.\n",
        "\n",
        "    * Practical Advantage: It is computationally efficient because it stops unnecessary splits early, saving time and resources during training.\n",
        "\n",
        "*   Post-Pruning (Pruning after Full Growth):\n",
        "\n",
        "    * Post-Pruning allows the tree to grow fully and then systematically removes or trims branches that do not add significant predictive power.\n",
        "\n",
        "    * Techniques include cost-complexity pruning, reduced error pruning, and pruning based on impurity thresholds.\n",
        "\n",
        "    * It refines the large tree by replacing some subtrees with leaf nodes in a bottom-up manner.\n",
        "\n",
        "    * Practical Advantage: It often results in better generalization and more accurate models because pruning decisions are based on actual performance of the fully grown tree."
      ],
      "metadata": {
        "id": "yCzoW7Vbqk2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Information Gain in Decision Trees is a metric that measures the effectiveness of a feature in splitting the dataset into classes. It quantifies how much knowing a feature reduces the uncertainty (entropy) of the target variable.\n",
        "\n",
        "    * In simple terms, Information Gain is calculated as:\n",
        "\n",
        "      Information Gain = Entropy(before split) - Weighted Entropy(after split)\n",
        "\n",
        "    * Entropy measures how mixed or impure a dataset is; it is high when classes are evenly distributed and low when mostly one class dominates.\n",
        "\n",
        "    * By splitting the data using a particular feature, the aim is to decrease entropy and make child nodes more homogeneous.\n",
        "\n",
        "*   Information Gain is important because:\n",
        "\n",
        "    * It helps decide the best feature and threshold to split the data at each node.\n",
        "\n",
        "    * The feature with the highest Information Gain is chosen for the split, leading to more pure nodes and better classification accuracy.\n",
        "\n",
        "    * It guides the tree-building process toward efficient and meaningful splits."
      ],
      "metadata": {
        "id": "HSP_nai2qk4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Here are some common real-world applications of Decision Trees along with their main advantages and limitations in those contexts:\n",
        "\n",
        "1. Loan Approval in Banking:\n",
        "\n",
        "    * Application:\n",
        "    \n",
        "      Banks use Decision Trees to decide loan approvals based on credit score, income, employment status, and loan history.\n",
        "\n",
        "    * Advantage:\n",
        "    \n",
        "      Provides clear, interpretable decisions that can be explained to customers and regulators.\n",
        "\n",
        "    * Limitation:\n",
        "      \n",
        "      Can be biased if training data is imbalanced or does not account for all risk factors.\n",
        "\n",
        "2. Medical Diagnosis:\n",
        "\n",
        "    * Application:\n",
        "    \n",
        "      Predicts whether a patient has a disease like diabetes using clinical data such as glucose level, BMI, and blood pressure.\n",
        "\n",
        "    * Advantage:\n",
        "    \n",
        "      Helps in early diagnosis and treatment with interpretable decision rules.\n",
        "\n",
        "    * Limitation:\n",
        "    \n",
        "      May overfit if too deep, leading to inaccurate predictions on unseen patient data.\n",
        "\n",
        "3. Customer Churn Prediction:\n",
        "\n",
        "    * Application:\n",
        "    \n",
        "      Predicts if a customer is likely to leave using behavioral data and purchase history.\n",
        "\n",
        "    * Advantage:\n",
        "    \n",
        "      Enables proactive retention strategies by identifying at-risk customers clearly.\n",
        "\n",
        "    * Limitation:\n",
        "    \n",
        "      Can be sensitive to noisy data and may require frequent retraining to stay current.\n",
        "\n",
        "4. Fraud Detection:\n",
        "\n",
        "    * Application:\n",
        "    \n",
        "      Detects fraudulent transactions by analyzing patterns in transaction data.\n",
        "\n",
        "    * Advantage:\n",
        "    \n",
        "      Provides transparency in alerts, aiding investigation teams.\n",
        "\n",
        "    * Limitation:\n",
        "    \n",
        "      May generate false positives or miss evolving fraud patterns without updates.\n",
        "\n",
        "5. Quality Control in Manufacturing:\n",
        "\n",
        "    * Application:\n",
        "    \n",
        "      Predicts defective products based on production variables.\n",
        "\n",
        "    * Advantage:\n",
        "    \n",
        "      Helps maintain quality standards and reduce waste with understandable rules.\n",
        "\n",
        "    * Limitation:\n",
        "    \n",
        "      Limited by the quality and granularity of sensor/production data available."
      ],
      "metadata": {
        "id": "EgmxZWvoqk7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model's accuracy and feature importances\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "6sXGqiTSqk96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data          # Features\n",
        "y = iris.target        # Target labels\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj8vqXbQTb_r",
        "outputId": "27ff9e94-db32-4680-9c05-90b2d8c6c9aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "27iQRtByqlAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully grown Decision Tree (max_depth=None)\n",
        "clf_full = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.4f}\")\n",
        "print(f\"Accuracy with full depth: {accuracy_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOWqT41uUsaW",
        "outputId": "d0a6d19a-72b4-4c75-bfcf-82a0ae9b992d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with full depth: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "CS8qEngNqlDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing dataset from URL (CSV format)\n",
        "url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(url, sep=r\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "\n",
        "# Process the raw data into features and target\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "# Split dataset into training and testing sets (70%-30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and compute MSE\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Feature names as per Boston dataset description\n",
        "feature_names = [\n",
        "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "    'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'\n",
        "]\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7kLpHHSVXtO",
        "outputId": "f08f42a6-6fc9-426d-da43-4e63185ee5c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 11.5880\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree's max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "Answer:\n",
        "\n",
        "* Below is the python program that loads the Iris dataset, uses GridSearchCV to tune the Decision Tree Classifier's max_depth and min_samples_split parameters, and prints the best parameters and resulting model accuracy:"
      ],
      "metadata": {
        "id": "sArkWUL2qlFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define Decision Tree classifier\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(dtree, param_grid, cv=5)\n",
        "\n",
        "# Fit GridSearch to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Predict on test set using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with tuned parameters: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldva3xLaWGZr",
        "outputId": "2d94e85e-d4db-4155-bd9e-ec82192cc979"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with tuned parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you're working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "*   To build a disease prediction Decision Tree model with a healthcare dataset containing mixed data types and missing values, follow this step-by-step process:\n",
        "\n",
        "1. Handle Missing Values:\n",
        "\n",
        "    Identify missing data patterns and proportions per feature.\n",
        "\n",
        "    Using appropriate imputation:\n",
        "\n",
        "    * For numerical features, fill missing values with mean, median, or use advanced methods like KNN imputation.\n",
        "\n",
        "    * For categorical features, fill with the mode or use a special 'missing' category.\n",
        "\n",
        "    Handle missingness carefully to avoid bias or data leakage.\n",
        "\n",
        "2. Encode Categorical Features:\n",
        "\n",
        "    Convert categorical variables to numeric representations compatible with Decision Trees.\n",
        "\n",
        "    Use:\n",
        "\n",
        "    * Label Encoding for ordinal categories.\n",
        "\n",
        "    * One-Hot Encoding for nominal categories.\n",
        "\n",
        "    Ensure encoding is consistent across training and test datasets.\n",
        "\n",
        "3. Train a Decision Tree Model:\n",
        "\n",
        "    Split data into training and test sets.\n",
        "\n",
        "    Initialize a DecisionTreeClassifier and train on the processed training data.\n",
        "\n",
        "    Use criteria like Gini impurity or entropy.\n",
        "\n",
        "    Handle class imbalance if present by adjusting class weights or resampling.\n",
        "\n",
        "4. Tune Hyperparameters:\n",
        "\n",
        "    Use GridSearchCV or RandomizedSearchCV to tune key hyperparameters such as:\n",
        "\n",
        "    * max_depth (tree depth limit)\n",
        "\n",
        "    * min_samples_split (minimum samples to split a node)\n",
        "\n",
        "    * min_samples_leaf (minimum samples at a leaf node)\n",
        "\n",
        "    * criterion (gini or entropy)\n",
        "\n",
        "    Perform cross-validation during tuning to avoid overfitting.\n",
        "\n",
        "5. Evaluate Model Performance:\n",
        "\n",
        "    Measure accuracy, precision, recall, F1-score, and ROC-AUC on the test set.\n",
        "\n",
        "    Use confusion matrix and feature importance for interpretability.\n",
        "\n",
        "    Optionally, perform calibration or threshold tuning for better clinical decision-making.\n",
        "\n",
        "6. This process add a business value as follow:\n",
        "\n",
        "    This process enables early and accurate disease detection, improving patient outcomes through timely intervention.\n",
        "\n",
        "    Supports clinical decision-making by providing interpretable rules.\n",
        "\n",
        "    Helps allocate medical resources efficiently by identifying high-risk patients.\n",
        "\n",
        "    Improves operational efficiency by automating risk stratification and reducing manual workload.\n",
        "\n",
        "    Facilitates personalized medicine and targeted treatments by identifying important predictors.\n",
        "\n",
        "    This process balances data quality, model performance, and interpretability essential for impactful healthcare predictive modeling."
      ],
      "metadata": {
        "id": "3YK8Gji9qlIG"
      }
    }
  ]
}