{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions : Feature Engineering"
      ],
      "metadata": {
        "id": "_E61Dkmcq6M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1. What is a parameter?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   A parameter in machine learning refers to an internal variable of a model that is learned or estimated directly from the training data during the training process. These parameters determine how the model transforms input features into outputs, such as making predictions on unseen data.\n",
        "*   The values of parameters are not set ahead of training; instead, they are continuously updated and optimized as the model learns from data.\n",
        "*   Parameters are crucial because they define the behavior of the trained model and directly influence its predictions and performance on new data.\n",
        "*   The parameters are distinct from hyperparameters, which are set before training and control aspects like the learning rate or model structure but are not themselves learned from the data.\n",
        "*   Examples of parameters include coefficients in linear regression, weights and biases in neural networks, and cluster centroids in clustering algorithms."
      ],
      "metadata": {
        "id": "xLW2Iajxq6Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2. What is correlation? What does negative correlation mean?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In Feature engineering, correlation is used to quantify the relationship between features and also between features and the target variable to guide the selection, removal, or transformation of features for more robust predictive modeling.\n",
        "*   Highly correlated features with the target variable are often selected for modeling, as they provide strong predictive power. Features with very low or zero correlation with the target can be removed, as they add little value to the prediction.\n",
        "*    If two features are highly correlated with each other (multicollinearity), one can often be dropped since they provide similar information, helping reduce model complexity and prevent overfitting.\n",
        "*   Common techniques for computing correlation include the Pearson coefficient (for numerical features), chi-squared test (for categorical features), and mutual information (for categorical and numeric features), among others.\n",
        "*   Correlation generally captures linear associations and may not effectively identify non-linear relationships.\n",
        "\n",
        "*  Key Points About Negative correlation:\n",
        "\n",
        "    * A negative correlation implies that there's an inverse linear relationship between variables. For example, if the correlation coefficient is -0.8, as one variable increases by 1 unit, the other tends to decrease by about 0.8 units on average.\n",
        "\n",
        "    * On scatter plots, negative correlation appears as a downward slope, where data points trend from the upper left to the lower right.\n",
        "    * Negative correlation is important in feature engineering as it can indicate features that move inversely with the target variable or with other features, helping in feature selection and simplification.\n",
        "    * Although perfect negative correlation is -1, most real-world correlations are imperfect, meaning the inverse relationship has some noise or variability. Negative correlation is different from no correlation (coefficient 0), where there is no observable linear relationship."
      ],
      "metadata": {
        "id": "hn1Vki9hq6Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Machine learning is a subset of artificial intelligence focused on developing algorithms that enable computers to learn patterns from data and make decisions or predictions without being explicitly programmed for each specific task. It enables systems to improve their performance on tasks through experience (data) and generalize to new, unseen data effectively.\n",
        "*   Main Components of Machine Learning:\n",
        "\n",
        "    * Data: The foundation of machine learning; includes features (input variables) and labels (output or target variables) for supervised learning. Data quality and quantity significantly impact model performance.\n",
        "\n",
        "    * Model/Algorithm: A mathematical function or system that learns patterns in the data. Examples include linear regression, decision trees, neural networks, etc.\n",
        "\n",
        "    * Training Process: The process where the model learns from data by adjusting its internal parameters to minimize errors and improve predictive accuracy.\n",
        "\n",
        "    * Features: Individual measurable properties or characteristics in the data that serve as input to the model. Feature engineering and selection play a crucial role in model success.\n",
        "\n",
        "    * Evaluation: Assessing model performance using metrics like accuracy, precision, recall, or mean squared error on validation or test data to ensure generalization.\n",
        "\n",
        "    * Prediction/Inference: The final step where the trained model is used to make decisions or predictions on new, unseen data."
      ],
      "metadata": {
        "id": "eqWzN6scq6V2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Answer:\n",
        "*   In machine learning, the loss value is a numerical measure of how well or poorly a model's predictions match the actual target values (ground truth). It quantifies the error or difference between the predicted outputs by the model and the true outputs.\n",
        "*   How Loss Value Helps Determine Model Quality\n",
        "\n",
        "    * Error Quantification: A lower loss value indicates that the model's predictions are closer to the actual values, reflecting better performance. Conversely, a higher loss means greater deviation and poorer accuracy.\n",
        "\n",
        "    * Guides Training: During training, the model parameters are adjusted to minimize the loss function. The loss provides a clear objective for the optimization process, helping the model learn patterns in data effectively.\n",
        "\n",
        "    * Comparison Metric: Loss values allow comparing different models or iterations of the same model under consistent criteria to decide which model is better.\n",
        "\n",
        "    * Early Stopping: Monitoring loss on validation data helps detect overfitting; if loss stops decreasing or worsens on validation data, training can be stopped to preserve model generalization.\n",
        "\n",
        "    * Choice of Loss Function: Different problems require different types of loss functions (e.g., mean squared error for regression or cross-entropy for classification), and selecting an appropriate loss function impacts how well the model learns and generalizes."
      ],
      "metadata": {
        "id": "ujTm5ogOq6Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5.  What are continuous and categorical variables?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Continuous Variables:\n",
        "\n",
        "    * Continuous variables represent numerical values that can take any value within a defined range.\n",
        "    \n",
        "    * They are measurable and can include fractional or decimal values. Examples include height, weight, temperature, and time.\n",
        "    \n",
        "    * Continuous variables are used in regression models and other algorithms that work with numeric inputs.\n",
        "\n",
        "*   Categorical Variables:\n",
        "\n",
        "    * Categorical variables represent distinct groups or categories and usually take on a limited, fixed number of possible values.\n",
        "    \n",
        "    * They are qualitative and describe characteristics or labels without inherent numeric meaning.\n",
        "    \n",
        "    * Examples include gender, color, type of product, or education level. Categorical variables can be:\n",
        "\n",
        "      Nominal: Categories with no natural order (e.g., colors like red, green, blue).\n",
        "\n",
        "      Ordinal: Categories with a meaningful order but not necessarily equal spacing (e.g., education levels: high school, bachelor's, master's)."
      ],
      "metadata": {
        "id": "Fv02bH2Cq6bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In machine learning, handling categorical variables involves converting them into a numerical format that algorithms can process effectively.\n",
        "The common techniques for encoding categorical variables include:\n",
        "\n",
        "    Label Encoding\n",
        "\n",
        "    * Assigns a unique integer to each category in the variable.\n",
        "\n",
        "    * Simple and memory-efficient.\n",
        "\n",
        "    * Best suited for ordinal variables but can mislead models if categories have no inherent order because it may imply a ranking.\n",
        "\n",
        "    One-Hot Encoding\n",
        "\n",
        "    * Creates binary columns for each category, where each column represents the presence (1) or absence (0) of a category.\n",
        "\n",
        "    * Preserves category distinctiveness without implying order.\n",
        "\n",
        "    * Can lead to high dimensionality, especially with many unique categories.\n",
        "\n",
        "    * Commonly used for nominal data and compatible with many algorithms.\n",
        "\n",
        "    Ordinal Encoding\n",
        "\n",
        "    * Encodes categories with an ordered integer sequence reflecting the inherent rank.\n",
        "\n",
        "    * Suitable for ordinal categorical variables such as \"low\", \"medium\", \"high\".\n",
        "\n",
        "    * The model understands the order but not the distance between categories.\n",
        "\n",
        "    Target Encoding (Mean Encoding)\n",
        "\n",
        "    * Replaces categories with a statistical measure (like mean of the target variable) grouped by category.\n",
        "\n",
        "    * Useful for high-cardinality variables but can introduce overfitting if not properly regularized.\n",
        "\n",
        "    Binary Encoding\n",
        "\n",
        "    * Converts each category into a binary code and splits binary digits into separate columns.\n",
        "\n",
        "    * Combines advantages of one-hot and label encoding.\n",
        "\n",
        "    * Reduces dimensionality for high-cardinality features.\n",
        "\n",
        "    Rare Label Encoding\n",
        "\n",
        "    * Groups infrequent categories into a single category labeled as \"Rare\" or \"Other\".\n",
        "\n",
        "    * Helps manage levels with very few instances to avoid overfitting and improve model robustness.\n",
        "\n",
        "    Effect Encoding (Deviation Encoding)\n",
        "\n",
        "    * Uses values 1, 0, and -1 to encode categories, handling multicollinearity better than dummy encoding.\n",
        "\n",
        "    * Mostly used with linear models for better coefficient interpretation."
      ],
      "metadata": {
        "id": "kk91K2uBq6dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7.  What do you mean by training and testing a dataset?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In machine learning, training and testing a dataset refer to different stages in the model development process aimed at ensuring it learns well and generalizes to new data.\n",
        "\n",
        "*   Training a Dataset:\n",
        "\n",
        "    * Training involves feeding the machine learning model with a portion of the available dataset, called the training set. During training, the model learns patterns, relationships, and parameters by analyzing the input features and their corresponding known outputs (labels).\n",
        "    \n",
        "    * The goal is to optimize the model so it can predict outcomes accurately by minimizing errors on this training data.\n",
        "\n",
        "*   Testing a Dataset:\n",
        "\n",
        "    * Testing involves evaluating the trained model's performance using a test set, which is a separate portion of the data not seen by the model during training.\n",
        "    \n",
        "    * The test data acts as new, unseen examples to verify how well the model generalizes and predicts outcomes on data outside of its training experience. This unbiased evaluation helps measure the model's real-world effectiveness and detect issues like overfitting."
      ],
      "metadata": {
        "id": "FrJSH0bRq6gY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8.  What is sklearn.preprocessing?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   The sklearn.preprocessing module in the scikit-learn library provides various utility functions and transformer classes to preprocess and transform raw feature data into a format that is more suitable for machine learning algorithms.\n",
        "\n",
        "*   Purpose of sklearn.preprocessing:\n",
        "\n",
        "    * It helps in scaling, centering, normalizing, and encoding data.\n",
        "\n",
        "    * It supports transformations such as converting categorical variables into numerical format, handling missing values, binarization, polynomial feature generation, and more.\n",
        "\n",
        "    * These transformations improve the model's ability to learn patterns, normalize data distributions, and ensure consistent input data representation.\n",
        "\n",
        "*   Common Functions and Classes in sklearn.preprocessing:\n",
        "\n",
        "\n",
        "    * StandardScaler: Scales features to have zero mean and unit variance.\n",
        "\n",
        "    * MinMaxScaler: Scales features to a given range, usually.\n",
        "\n",
        "    * OneHotEncoder: Encodes categorical features as one-hot numeric arrays.\n",
        "\n",
        "    * LabelEncoder: Converts categorical labels to integer form.\n",
        "\n",
        "    * Normalizer: Normalizes samples individually to unit norm.\n",
        "\n",
        "    * PolynomialFeatures: Generates polynomial and interaction features.\n",
        "\n",
        "    * Binarizer: Converts numerical values to binary values based on a threshold.\n",
        "\n",
        "    * SimpleImputer: Imputes missing values (available in related modules) but often part of preprocessing pipelines."
      ],
      "metadata": {
        "id": "Cs_bvIqyq6kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9.  What is a Test set?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In machine learning, a test set is a portion of the dataset that is kept separate and untouched during the training and tuning phases of the model development process.\n",
        "\n",
        "*   Definition of Test Set:\n",
        "\n",
        "    * It consists of data examples that the model has never seen before.\n",
        "\n",
        "    * The purpose of the test set is to provide an unbiased evaluation of the final trained model's performance.\n",
        "\n",
        "    * By using the test set, we can measure how well the model generalizes to new, unseen data, simulating real-world application scenarios.\n",
        "\n",
        "*   Importance of Test Set:\n",
        "\n",
        "    * Helps assess the accuracy and robustness of the model after training.\n",
        "\n",
        "    * Ensures that the model is not just memorizing training data (overfitting) but learning to generalize.\n",
        "\n",
        "    * Serves as the final checkpoint before deploying the model for actual use.\n",
        "\n",
        "*   Typical Data Split:\n",
        "\n",
        "    * The dataset is usually divided into training, validation, and test sets.\n",
        "\n",
        "    * The test set often comprises 10-30% of the total data, depending on the dataset size and project requirements."
      ],
      "metadata": {
        "id": "wH2VKe7Pq6mR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In Python, the common way to split a dataset into training and testing sets for model fitting is by using the train_test_split() function from the sklearn.model_selection module. This function allows to divide the data into subsets so that one part is used for training the model and the other part is used for evaluating its performance on unseen data.\n",
        "*   Below are the steps to split data using train_test_split():\n",
        "\n",
        "    * Import the function:\n",
        "\n",
        "    * Prepare data into features (X) and target labels (y).\n",
        "\n",
        "    * Split the data using the function, specifying parameters like test size and random state for reproducibility\n",
        "\n",
        "    * Parameters:\n",
        "\n",
        "      test_size=0.2: 20% of data is reserved for testing, and 80% for training.\n",
        "\n",
        "      random_state=42: ensures the split is reproducible every time.\n",
        "\n",
        "      Optional stratify parameter ensures class distribution is preserved for classification tasks.\n",
        "\n",
        "\n",
        "    * This split ensures the model learns from the training set and its performance is objectively evaluated on the test set.\n",
        "\n",
        "*   How to Approach a Machine Learning Problem:\n",
        "\n",
        "    Understand the Problem and Data\n",
        "\n",
        "    * Define the problem clearly.\n",
        "\n",
        "    * Understand the data sources, data types, and what the target variable is.\n",
        "\n",
        "    Data Collection and Preparation\n",
        "\n",
        "    * Gather and clean data (handle missing values, remove duplicates).\n",
        "\n",
        "    * Perform exploratory data analysis to understand distributions and relationships.\n",
        "\n",
        "    * Apply feature engineering and encoding for categorical variables.\n",
        "\n",
        "    Split the Data\n",
        "\n",
        "    * Divide data into training, validation (optional), and testing sets.\n",
        "\n",
        "    Choose Algorithms and Models\n",
        "\n",
        "    * Select one or more models suitable for the problem (e.g., regression, classification).\n",
        "\n",
        "    Train the Model\n",
        "\n",
        "    * Fit models on the training data.\n",
        "\n",
        "    Evaluate the Model\n",
        "\n",
        "    * Validate performance on validation or test data using metrics like accuracy, precision, recall, F1-score, RMSE, etc.\n",
        "\n",
        "    Tune Hyperparameters\n",
        "\n",
        "    * Adjust parameters to optimize model performance using cross-validation or grid search.\n",
        "\n",
        "    Deploy and Monitor\n",
        "\n",
        "    * Deploy the final model to production.\n",
        "\n",
        "    Monitor model performance and update it as necessary.\n",
        "    \n",
        "*   The below python code demonstrate that how do we split data for model fitting (training and testing):"
      ],
      "metadata": {
        "id": "Lz-ovFznq6ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating a small dataset using dictionary\n",
        "data = {\n",
        "    'feature1': [5, 10, 15, 20, 25, 30],\n",
        "    'feature2': [50, 40, 30, 20, 10, 0],\n",
        "    'target': [1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Splitting the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBH75TPrlGb3",
        "outputId": "d66ef3c0-5e15-41ae-8f5c-cc7cdba3266b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:    feature1  feature2\n",
            "5        30         0\n",
            "2        15        30\n",
            "4        25        10\n",
            "3        20        20\n",
            "X_test:    feature1  feature2\n",
            "0         5        50\n",
            "1        10        40\n",
            "y_train: 5    0\n",
            "2    1\n",
            "4    1\n",
            "3    0\n",
            "Name: target, dtype: int64\n",
            "y_test: 0    1\n",
            "1    0\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11.  Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential for several important reasons:\n",
        "\n",
        "*   Reasons to Perform EDA Before Modeling\n",
        "    \n",
        "    Understanding the Dataset\n",
        "\n",
        "    * EDA helps reveal the structure of the data, including the number of features, data types, and distribution of values. This helps grasp what kind of data you are working with and the underlying patterns.\n",
        "\n",
        "    Identifying Data Quality Issues\n",
        "\n",
        "    * EDA detects missing values, duplicates, inconsistencies, and errors in the dataset that could degrade model performance. Cleaning such issues early is crucial for trustworthy predictions.\n",
        "\n",
        "    Spotting Outliers and Anomalies\n",
        "\n",
        "    * Outliers can significantly skew the results and mislead learning algorithms. Visual and statistical techniques during EDA uncover these anomalies for handling or removal.\n",
        "\n",
        "    Discovering Relationships and Patterns\n",
        "\n",
        "    * EDA facilitates identifying correlations between features and their relationship with the target variable. This insight helps in feature selection and engineering, guiding the choice of predictors for the model.\n",
        "\n",
        "    Choosing the Right Model and Transformations\n",
        "\n",
        "    * Understanding the data distribution and feature types through EDA guides selecting suitable machine learning algorithms and appropriate data preprocessing or transformations.\n",
        "\n",
        "    Testing Assumptions\n",
        "\n",
        "    * EDA helps verify assumptions required by specific models, such as normality or linearity, ensuring the chosen modeling techniques are valid.\n",
        "\n",
        "*   In short, EDA bridges the gap between raw data and meaningful insights that empower building robust, accurate, and interpretable machine learning models. Skipping or insufficient EDA increases the risk of poor model performance due to faulty or suboptimal data preparation."
      ],
      "metadata": {
        "id": "oMAb0Jjtq6rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 12. What is correlation?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In Feature engineering, correlation is used to quantify the relationship between features and also between features and the target variable to guide the selection, removal, or transformation of features for more robust predictive modeling.\n",
        "*   Highly correlated features with the target variable are often selected for modeling, as they provide strong predictive power. Features with very low or zero correlation with the target can be removed, as they add little value to the prediction.\n",
        "*    If two features are highly correlated with each other (multicollinearity), one can often be dropped since they provide similar information, helping reduce model complexity and prevent overfitting.\n",
        "*   Common techniques for computing correlation include the Pearson coefficient (for numerical features), chi-squared test (for categorical features), and mutual information (for categorical and numeric features), among others.\n",
        "*   Correlation generally captures linear associations and may not effectively identify non-linear relationships."
      ],
      "metadata": {
        "id": "lhgffm4wq6tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 13. What does negative correlation mean?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*  Key Points AboutNegative correlation:\n",
        "\n",
        "    * A negative correlation implies that there's an inverse linear relationship between variables. For example, if the correlation coefficient is -0.8, as one variable increases by 1 unit, the other tends to decrease by about 0.8 units on average.\n",
        "\n",
        "    * On scatter plots, negative correlation appears as a downward slope, where data points trend from the upper left to the lower right.\n",
        "    * Negative correlation is important in feature engineering as it can indicate features that move inversely with the target variable or with other features, helping in feature selection and simplification.\n",
        "    * Although perfect negative correlation is -1, most real-world correlations are imperfect, meaning the inverse relationship has some noise or variability. Negative correlation is different from no correlation (coefficient 0), where there is no observable linear relationship."
      ],
      "metadata": {
        "id": "53_32KtRq6w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 14.  How can you find correlation between variables in Python?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In Python, we can find the correlation between variables using several methods, especially with libraries like Pandas and NumPy.\n",
        "Using Pandas\n",
        "*   If we have a DataFrame, you can use the .corr() method to get the correlation matrix for all numeric columns. It is demonstrated using below python code:"
      ],
      "metadata": {
        "id": "u8TXYsCYq6zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'var1': [10, 20, 30, 40, 50],\n",
        "    'var2': [15, 25, 35, 45, 55],\n",
        "    'var3': [50, 40, 30, 20, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XDm9CPi3wxy",
        "outputId": "a9b31c99-9f20-40dc-d89f-4ccbb968df68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      var1  var2  var3\n",
            "var1   1.0   1.0  -1.0\n",
            "var2   1.0   1.0  -1.0\n",
            "var3  -1.0  -1.0   1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Using NumPy:\n",
        "\n",
        "    For two arrays or lists, use np.corrcoef():"
      ],
      "metadata": {
        "id": "xoHW79HJq61u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([10, 20, 30, 40, 50])\n",
        "y = np.array([15, 25, 35, 45, 55])\n",
        "\n",
        "correlation = np.corrcoef(x, y)[0, 1]\n",
        "print(\"Correlation coefficient:\", correlation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UNs1WQD4Dbk",
        "outputId": "b4c5fb6e-60f7-44aa-ace4-d20912fb6abb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation coefficient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Using SciPy for Pearson Correlation and p-value"
      ],
      "metadata": {
        "id": "WEQES7_qq64l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "x = [10, 20, 30, 40, 50]\n",
        "y = [15, 25, 35, 45, 55]\n",
        "\n",
        "corr, p_value = pearsonr(x, y)\n",
        "print(\"Pearson correlation:\", corr)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seu2T2yP4WJV",
        "outputId": "d7736edd-a1a8-40c4-e920-6b9820ee99d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation: 0.9999999999999996\n",
            "P-value: 1.1234123376434879e-23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 15.  What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Causation\n",
        "\n",
        "    * Causation means that a change in one variable directly causes a change in another variable; there is a cause-and-effect relationship between the two.\n",
        "    \n",
        "    * When one event (the cause) occurs, it brings about another event (the effect). For example, heavy rainfall causes water levels in rivers to rise, leading to flooding.\n",
        "\n",
        "*   Difference between correlation and causation:\n",
        "\n",
        "    * Correlation is the relationship where two variables move together, with one changing as the other changes, while causation is when one variable directly causes the change in another variable.\n",
        "\n",
        "    * Correlation means variables are associated with each other, while causation means one variable is responsible for the effect on the other.\n",
        "\n",
        "    * Correlation shows a pattern of co-movement, while causation explains a cause-and-effect mechanism.\n",
        "\n",
        "    * Correlation can be coincidental or due to a third factor, while causation implies a direct influence.\n",
        "\n",
        "    * Correlation with ice cream sales and sunburn shows both increase in summer, while causation with sun exposure causing sunburn means one directly leads to the other.\n",
        "\n",
        "*   Example:\n",
        "\n",
        "    * Correlation: There is a correlation between ice cream sales and sunburn cases because both increase during hot weather. However, buying ice cream does not cause sunburn. Instead, the third variable sunny weather influences both.\n",
        "\n",
        "    * Causation: Smoking causes lung cancer. The act of smoking directly leads to cellular changes resulting in cancer."
      ],
      "metadata": {
        "id": "DmN2aRRPq66p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Optimizer:\n",
        "\n",
        "    * An optimizer is an algorithm used in machine learning and numerical methods to adjust the parameters of a model in order to minimize (or maximize) an objective function, typically a loss function. The goal is to find the best parameter values that reduce prediction error and improve model performance.\n",
        "\n",
        "*   Different Types of Optimizers with Examples\n",
        "\n",
        "    1. Gradient Descent (GD):\n",
        "\n",
        "    * How it works: Computes the gradient of the loss function with respect to parameters and updates parameters in the opposite direction to the gradient to minimize the loss.\n",
        "\n",
        "    * Example: Updating weights in linear regression by subtracting a fraction (learning rate) of the gradient.\n",
        "\n",
        "    * Use case: Basis for many ML models including linear and logistic regression.\n",
        "\n",
        "    2. Stochastic Gradient Descent (SGD):\n",
        "\n",
        "    * How it works: Similar to GD, but updates parameters using one sample at a time instead of the whole dataset.\n",
        "\n",
        "    * Example: Randomly picking one training example, computing gradient, updating weights iteratively.\n",
        "\n",
        "    * Use case: Often used in large datasets for faster convergence.\n",
        "\n",
        "    3. Mini-Batch Gradient Descent:\n",
        "\n",
        "    * How it works: Compromises between GD and SGD by using small batches (subset) of data for each update.\n",
        "\n",
        "    * Example: Uses batches of 32 or 64 examples to compute gradient and update parameters.\n",
        "\n",
        "    * Use case: Common in deep learning as it balances speed and stability.\n",
        "\n",
        "    4. Momentum:\n",
        "\n",
        "    * How it works: Adds a fraction of the previous update vector to the current update to accelerate convergence and reduce oscillations.\n",
        "\n",
        "    * Example: Helps to glide over flat regions and avoid local minima.\n",
        "\n",
        "    * Use case: Improves SGD performance in deep learning.\n",
        "\n",
        "    5. RMSProp:\n",
        "\n",
        "    * How it works: Adapts the learning rate for each parameter by dividing the gradient by a running average of recent magnitudes.\n",
        "\n",
        "    * Example: Allows larger updates for infrequent parameters, smaller for frequent ones.\n",
        "\n",
        "    * Use case: Popular for training recurrent neural networks.\n",
        "\n",
        "    6. Adam Optimizer:\n",
        "\n",
        "    * How it works: Combines momentum and RMSProp by maintaining running averages of both gradients and their squares.\n",
        "\n",
        "    * Example: Adjusts learning rates adaptively for each parameter using bias correction.\n",
        "\n",
        "    * Use case: Default optimizer for many deep learning models due to efficiency and ease of use.\n",
        "\n",
        "    7. Genetic Algorithms (GA):\n",
        "\n",
        "    * How it works: Inspired by biological evolution using mutation, crossover, and selection to optimize parameters.\n",
        "\n",
        "    * Example: Population of candidate solutions evolves over generations to find optimum.\n",
        "\n",
        "    * Use case: Suitable for complex or poorly understood optimization problems."
      ],
      "metadata": {
        "id": "o3yt8G_U4cw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 17. What is sklearn.linear_model ?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   sklearn.linear_model:\n",
        "\n",
        "    * The sklearn.linear_model module in scikit-learn is a collection of linear models used for regression and classification tasks.\n",
        "    * These models assume a linear relationship between the input features and the target variable.\n",
        "    * The module provides easy-to-use classes and functions to fit linear models, make predictions, and evaluate model performance.\n",
        "\n",
        "*   Key Features and Common Models in sklearn.linear_model\n",
        "\n",
        "    LinearRegression\n",
        "\n",
        "    * Fits a linear model by minimizing residual sum of squares between observed targets and predicted values.\n",
        "\n",
        "    * use: Predicting house prices based on features like size and location.\n",
        "\n",
        "    * Instantiated as LinearRegression().\n",
        "\n",
        "    LogisticRegression\n",
        "\n",
        "    * Classification model that predicts the probability of a categorical outcome using a logistic function.\n",
        "\n",
        "    * Useful for binary or multiclass classification problems.\n",
        "\n",
        "    * Supports regularization and multiple solvers.\n",
        "\n",
        "    * Instantiated as LogisticRegression().\n",
        "\n",
        "    Ridge Regression and Lasso\n",
        "\n",
        "    * Extensions of linear regression that include regularization terms to avoid overfitting.\n",
        "\n",
        "    * Ridge Adds L2 penalty, Lasso adds L1 penalty.\n",
        "\n",
        "    * Useful for feature selection and improving generalization.\n",
        "\n",
        "    SGDRegressor and SGDClassifier\n",
        "\n",
        "    * Models trained using Stochastic Gradient Descent for large-scale learning.\n",
        "\n",
        "    ElasticNet\n",
        "\n",
        "    * Combines penalties of both Lasso and Ridge regression.\n",
        "\n",
        "*   Example: Linear Regression with sklearn.linear_model:"
      ],
      "metadata": {
        "id": "VoAwqjwN4c0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit model on training data (X_train, y_train)\n",
        "model.fit(X_train, y_train)   #Using values from question 10\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Access intercept and coefficients\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaTPd7TS8KRk",
        "outputId": "6d2e473f-40a3-47fb-ef8b-c611360e0df7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 0.44\n",
            "Coefficients: [-0.008  0.016]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   The model.fit() method in scikit-learn is used to train a machine learning model. It takes the training data as input and allows the model to learn the underlying patterns by adjusting its internal parameters.\n",
        "\n",
        "*   What does model.fit() do?\n",
        "\n",
        "    * It receives the feature matrix X and the target vector y as input.\n",
        "\n",
        "    * It computes and optimizes the model parameters (such as weights for linear models) based on the input data.\n",
        "\n",
        "    * It minimizes the loss function (error between predicted and actual target values).\n",
        "\n",
        "    * After fitting, the model stores the learned parameters to be used for predictions on new data.\n",
        "\n",
        "*   Arguments required by model.fit()\n",
        "    \n",
        "    * X: The input data/features; usually a 2D array or DataFrame of shape (n_samples,n_features).\n",
        "\n",
        "    * y: The target/labels; usually a 1D array or Series of shape (n_samples,).\n",
        "\n",
        "    * Optional: Some models accept additional arguments like sample weights."
      ],
      "metadata": {
        "id": "qVBOmSDz4c5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 19.  What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   The model.predict() method in scikit-learn is used to make predictions using a trained model on new, unseen data after the model has been fitted.\n",
        "\n",
        "    * It takes new input data usually a feature matrix Xnew and applies the learned model parameters to predict the target values.\n",
        "\n",
        "    * For classification models, it predicts class labels for the input samples.\n",
        "\n",
        "    * For regression models, it predicts continuous values based on the input features.\n",
        "\n",
        "*   Arguments required by model.predict():\n",
        "\n",
        "    * X: The new data/features on which predictions are to be made; typically a 2D array-like structure (NumPy array, list, or Pandas DataFrame).\n",
        "\n",
        "    * No target variable is required since it just performs inference.\n",
        "\n",
        "*   Below python code demonstrate the use of model.predict() in machine learning."
      ],
      "metadata": {
        "id": "GtRDB5WC4c92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Training data\n",
        "X_train = [[1], [2], [3], [4]]\n",
        "y_train = [2, 4, 6, 8]\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = [[5], [6]]\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "print(predictions)  # Output could be [10., 12.]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtIfSFrbB5_V",
        "outputId": "f6e0721a-8aff-47e6-b06b-7bfeb789d8ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10. 12.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 20. What are continuous and categorical variables?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Continuous Variables:\n",
        "\n",
        "    * Continuous variables represent numerical values that can take any value within a defined range.\n",
        "    \n",
        "    * They are measurable and can include fractional or decimal values. Examples include height, weight, temperature, and time.\n",
        "    \n",
        "    * Continuous variables are used in regression models and other algorithms that work with numeric inputs.\n",
        "\n",
        "*   Categorical Variables:\n",
        "\n",
        "    * Categorical variables represent distinct groups or categories and usually take on a limited, fixed number of possible values.\n",
        "    \n",
        "    * They are qualitative and describe characteristics or labels without inherent numeric meaning.\n",
        "    \n",
        "    * Examples include gender, color, type of product, or education level. Categorical variables can be:\n",
        "\n",
        "      Nominal: Categories with no natural order (e.g., colors like red, green, blue).\n",
        "\n",
        "      Ordinal: Categories with a meaningful order but not necessarily equal spacing (e.g., education levels: high school, bachelor's, master's)."
      ],
      "metadata": {
        "id": "hMpmoWwi4dCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Feature Scaling:\n",
        "\n",
        "    * Feature scaling is a data preprocessing technique in machine learning where numerical features are transformed to a common scale or range.\n",
        "    * This process ensures that different features contribute equally to the model by adjusting their magnitudes to be comparable. Without feature scaling, features with larger ranges or units can dominate the model's learning process.\n",
        "\n",
        "*   How Does Feature Scaling Help in Machine Learning?\n",
        "\n",
        "    Improves Algorithm Performance and Convergence:\n",
        "\n",
        "    * Many machine learning algorithms, especially those based on gradient descent (e.g., linear regression, logistic regression, neural networks), converge faster and more reliably when the input features are on similar scales. Feature scaling helps the optimizer update all parameters uniformly.\n",
        "\n",
        "    Balances Influence of Features:\n",
        "\n",
        "    * Features with vastly different ranges (e.g., age 0-100 vs income in thousands) can disproportionately affect distance calculations or weight assignments. Scaling ensures no feature dominates due to its numerical magnitude.\n",
        "\n",
        "    Optimizes Distance-Based Algorithms:\n",
        "\n",
        "    * Algorithms like k-Nearest Neighbors (k-NN), K-means clustering, and Support Vector Machines (SVM) rely on distance metrics. Feature scaling prevents variables with larger scales from skewing the distance calculation, allowing all features to influence the model fairly.\n",
        "\n",
        "    Prevents Numerical Instability and Bias:\n",
        "\n",
        "    * Scaling reduces risks of numerical overflow or instability and ensures that regularization techniques penalize features appropriately when ranges vary widely.\n",
        "\n",
        "    Makes Results More Interpretable:\n",
        "\n",
        "    * Standardized features centered around zero with unit variance make it easier to interpret the importance and impact of individual features in models like linear regression.\n",
        "\n",
        "*   Common Methods of Feature Scaling:\n",
        "\n",
        "\n",
        "    * Normalization (Min-Max Scaling): Scales data to a fixed range, typically 0 to 1.\n",
        "\n",
        "    * Standardization (Z-score): Scales data to have mean 0 and standard deviation 1.\n",
        "\n",
        "    * Robust Scaling: Uses median and interquartile range to reduce influence of outliers."
      ],
      "metadata": {
        "id": "YtIMHwRi4dGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 22. How do we perform scaling in Python?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   We can perform feature scaling in Python primarily using the preprocessing module of the scikit-learn library, which provides ready-to-use classes like StandardScaler, MinMaxScaler, and RobustScaler.\n",
        "*   Common Methods and Example Code:\n",
        "\n",
        "    1. Standardization (Z-score Normalization)\n",
        "      \n",
        "    * Centers features by removing the mean and scales to unit variance.\n",
        "\n",
        "    * Useful when data follows a Gaussian distribution.\n",
        "\n",
        "    2. Min-Max Scaling (Normalization)\n",
        "\n",
        "    * Scales features to a fixed range, usually 0 to 1.\n",
        "\n",
        "    * Preserves the distribution shape but sensitive to outliers.\n",
        "\n",
        "    3. Robust Scaling:\n",
        "\n",
        "    * Uses median and interquartile range.\n",
        "\n",
        "    * Reduces the influence of outliers.\n",
        "\n",
        "*   The below python code demonstrate the scaling in python:"
      ],
      "metadata": {
        "id": "bB79s5q74dK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Standardization (Z-score Normalization)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'feature1': [10, 20, 15, 30, 45], 'feature2': [100, 150, 120, 200, 230]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Min-Max Scaling (Normalization)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)\n",
        "\n",
        "\n",
        "# 3. Robust Scaling\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJv2SdnFPrp",
        "outputId": "e9356c09-d69b-4b2c-95e5-44fe3ad95609"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature1  feature2\n",
            "0 -1.128152 -1.235080\n",
            "1 -0.322329 -0.205847\n",
            "2 -0.725241 -0.823387\n",
            "3  0.483494  0.823387\n",
            "4  1.692228  1.440927\n",
            "   feature1  feature2\n",
            "0  0.000000  0.000000\n",
            "1  0.285714  0.384615\n",
            "2  0.142857  0.153846\n",
            "3  0.571429  0.769231\n",
            "4  1.000000  1.000000\n",
            "   feature1  feature2\n",
            "0 -0.666667    -0.625\n",
            "1  0.000000     0.000\n",
            "2 -0.333333    -0.375\n",
            "3  0.666667     0.625\n",
            "4  1.666667     1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 23. What is sklearn.preprocessing?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   The sklearn.preprocessing module in the scikit-learn library provides various utility functions and transformer classes to preprocess and transform raw feature data into a format that is more suitable for machine learning algorithms.\n",
        "\n",
        "*   Purpose of sklearn.preprocessing:\n",
        "\n",
        "    * It helps in scaling, centering, normalizing, and encoding data.\n",
        "\n",
        "    * It supports transformations such as converting categorical variables into numerical format, handling missing values, binarization, polynomial feature generation, and more.\n",
        "\n",
        "    * These transformations improve the model's ability to learn patterns, normalize data distributions, and ensure consistent input data representation.\n",
        "\n",
        "*   Common Functions and Classes in sklearn.preprocessing:\n",
        "\n",
        "\n",
        "    * StandardScaler: Scales features to have zero mean and unit variance.\n",
        "\n",
        "    * MinMaxScaler: Scales features to a given range, usually.\n",
        "\n",
        "    * OneHotEncoder: Encodes categorical features as one-hot numeric arrays.\n",
        "\n",
        "    * LabelEncoder: Converts categorical labels to integer form.\n",
        "\n",
        "    * Normalizer: Normalizes samples individually to unit norm.\n",
        "\n",
        "    * PolynomialFeatures: Generates polynomial and interaction features.\n",
        "\n",
        "    * Binarizer: Converts numerical values to binary values based on a threshold.\n",
        "\n",
        "    * SimpleImputer: Imputes missing values (available in related modules) but often part of preprocessing pipelines."
      ],
      "metadata": {
        "id": "G8FpFnK34dN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In Python, the common way to split a dataset into training and testing sets for model fitting is by using the train_test_split() function from the sklearn.model_selection module. This function allows to divide the data into subsets so that one part is used for training the model and the other part is used for evaluating its performance on unseen data.\n",
        "*   Below are the steps to split data using train_test_split():\n",
        "\n",
        "    * Import the function:\n",
        "\n",
        "    * Prepare data into features (X) and target labels (y).\n",
        "\n",
        "    * Split the data using the function, specifying parameters like test size and random state for reproducibility\n",
        "\n",
        "    * Parameters:\n",
        "\n",
        "      test_size=0.2: 20% of data is reserved for testing, and 80% for training.\n",
        "\n",
        "      random_state=42: ensures the split is reproducible every time.\n",
        "\n",
        "      Optional stratify parameter ensures class distribution is preserved for classification tasks.\n",
        "\n",
        "\n",
        "    * This split ensures the model learns from the training set and its performance is objectively evaluated on the test set."
      ],
      "metadata": {
        "id": "QQFU4bPR4eMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating a small dataset using dictionary\n",
        "data = {\n",
        "    'feature1': [10, 20, 30, 40, 50, 60],\n",
        "    'feature2': [50, 100, 150, 200, 250, 300],\n",
        "    'target': [1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Splitting the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk7Qz1bSGi_D",
        "outputId": "7968b74c-2d00-46cc-db42-5395240fe0f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:    feature1  feature2\n",
            "5        60       300\n",
            "2        30       150\n",
            "4        50       250\n",
            "3        40       200\n",
            "X_test:    feature1  feature2\n",
            "0        10        50\n",
            "1        20       100\n",
            "y_train: 5    0\n",
            "2    1\n",
            "4    1\n",
            "3    0\n",
            "Name: target, dtype: int64\n",
            "y_test: 0    1\n",
            "1    0\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 25. Explain data encoding?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Data Encoding:\n",
        "\n",
        "    * Data encoding in machine learning refers to the process of converting categorical variables (non-numeric data such as labels, categories, or text) into numerical representations that machine learning algorithms can interpret and work with effectively. Since most algorithms require numerical input for mathematical computations, encoding categorical data into numbers is essential.\n",
        "\n",
        "*   Important of Data Encoding:\n",
        "\n",
        "    * Machine learning models generally perform numerical computations and cannot directly process text or categorical data.\n",
        "\n",
        "    * Encoding transforms categorical data into a numeric form, enabling models to learn and find patterns.\n",
        "\n",
        "    * Proper encoding can improve model accuracy and efficiency.\n",
        "\n",
        "    * Incorrect or absent encoding can lead to misleading or poor models.\n",
        "\n",
        "*  Common Types of Data Encoding\n",
        "\n",
        "    Label Encoding:\n",
        "\n",
        "    * Assigns a unique integer to each category.\n",
        "\n",
        "      Example: ['Red', 'Green', 'Blue']\n",
        "      \n",
        "      Suitable for ordinal categories.\n",
        "\n",
        "    One-Hot Encoding:\n",
        "\n",
        "    * Creates binary columns for each category, denoting presence (1) or absence (0).\n",
        "\n",
        "      Example: ['Red', 'Green', 'Blue']\n",
        "\n",
        "      Suitable for nominal categories (no order).\n",
        "\n",
        "    Ordinal Encoding:\n",
        "\n",
        "    * Encodes categories with a meaningful order as integers.\n",
        "\n",
        "      Example: ['Low', 'Medium', 'High']\n",
        "\n",
        "    Binary Encoding:\n",
        "\n",
        "    * Converts categories to binary codes split into separate columns, useful for high-cardinality data."
      ],
      "metadata": {
        "id": "CVNYkHZc4eQH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evcPUfkEIawf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}