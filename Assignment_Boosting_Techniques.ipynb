{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Techniques | Assignment"
      ],
      "metadata": {
        "id": "L-0gAHafhacr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1:  What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Boosting in Machine Learning is a powerful ensemble technique that combines multiple weak learners to create a strong predictive model. Unlike other ensemble methods like bagging that train models independently and in parallel, boosting trains weak learners sequentially, with each new model focusing on correcting the errors made by its predecessors. This iterative approach allows the overall model to gradually improve predictive accuracy by paying more attention to difficult-to-predict samples.\n",
        "\n",
        "*   The boosting process typically starts by assigning equal weights to all training samples and training an initial weak learner, often a shallow decision tree. After evaluating this model's predictions, the algorithm increases the weights of misclassified samples so that the next learner focuses more on these challenging cases. This cycle of weight adjustment and training continues until a stopping criterion is met, such as a specified number of iterations or minimal training error.\n",
        "\n",
        "*   Boosting improves weak learners by:\n",
        "\n",
        "    * Reducing bias:\n",
        "    \n",
        "      Each weak learner addresses the shortcomings of the previous ones, thereby progressively minimizing the overall error.\n",
        "\n",
        "    * Focusing learning:\n",
        "    \n",
        "      By emphasizing harder examples, boosting concentrates the model's capacity where it is most needed.\n",
        "\n",
        "    * Combining weak models:\n",
        "    \n",
        "      The final strong learner aggregates predictions from all weak learners, usually using weighted voting or averaging, resulting in significantly enhanced accuracy compared to any individual weak learner.\n",
        "\n",
        "*   Popular boosting algorithms include AdaBoost, which adaptively adjusts sample weights, Gradient Boosting, which fits subsequent models to residual errors, and XGBoost, an optimized implementation with regularization for improved speed and precision.\n",
        "\n",
        "*   In summary, boosting systematically transforms a sequence of simple, weak models into a highly accurate and robust ensemble by iteratively correcting mistakes and focusing on challenging data points. This makes boosting especially effective for complex classification and regression tasks where individual models alone may fall short."
      ],
      "metadata": {
        "id": "uI3R4kfkhc-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   AdaBoost (Adaptive Boosting) and Gradient Boosting are both powerful boosting algorithms used to convert weak learners into a stronger ensemble model. However, they differ fundamentally in how the models are trained and how errors are corrected.\n",
        "\n",
        "*   Training Process:\n",
        "\n",
        "    * AdaBoost:\n",
        "\n",
        "      AdaBoost trains models sequentially by adaptively adjusting the weights of training samples based on the errors of the previous learner. Initially, every sample has an equal weight. After each weak learner is trained, AdaBoost increases the weights of the misclassified samples so that the subsequent learner focuses more on these hard-to-classify instances. Each weak learner is assigned a weight based on its accuracy, and the final prediction is a weighted majority vote of all learners.\n",
        "\n",
        "    * Gradient Boosting:\n",
        "\n",
        "      Gradient Boosting also trains models sequentially, but instead of adjusting sample weights, it fits each new weak learner to the residual errors (the difference between actual and predicted values) of the combined ensemble so far. It treats boosting as an optimization problem that minimizes a differentiable loss function by using gradient descent. Each successive model corrects the errors of the aggregate model by learning the negative gradient of the loss function with respect to prediction errors.\n",
        "\n",
        "*   Error Correction Approach:\n",
        "\n",
        "    * AdaBoost:\n",
        "    \n",
        "      Focuses on reweighting misclassified samples and emphasizes them in the upcoming learner to reduce classification errors. It uses an exponential loss function that penalizes misclassification strongly.\n",
        "\n",
        "    * Gradient Boosting:\n",
        "\n",
        "      Directly minimizes a chosen loss function (e.g., squared error for regression, log-loss for classification) by fitting learners to the residuals, essentially descending the loss function's gradient to refine the ensemble.\n",
        "\n",
        "*   Base Learners:\n",
        "\n",
        "    * AdaBoost:\n",
        "\n",
        "      Typically employs simple weak learners such as decision stumps (trees with one split).\n",
        "\n",
        "    * Gradient Boosting:\n",
        "\n",
        "      Usually uses deeper decision trees as base learners to capture more complex relationships.\n",
        "\n",
        "*   Flexibility:\n",
        "\n",
        "    * AdaBoost:\n",
        "      \n",
        "      Primarily designed for binary classification and less flexible in terms of loss functions.\n",
        "\n",
        "    * Gradient Boosting:\n",
        "\n",
        "      Supports a variety of loss functions (regression, classification, ranking, custom losses), making it more versatile.\n",
        "\n",
        "*   Sensitivity to Noise:\n",
        "\n",
        "    * AdaBoost:\n",
        "\n",
        "      More sensitive to noisy data and outliers since it increases weights for misclassified points, potentially emphasizing noise.\n",
        "\n",
        "    * Gradient Boosting:\n",
        "\n",
        "      Generally more robust to noise due to gradient-based optimization and flexibility in loss choice."
      ],
      "metadata": {
        "id": "CXanc7VyhdEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Regularization in XGBoost helps prevent overfitting and improves model generalization by controlling the complexity of the learned trees. It achieves this by adding penalty terms to the objective function that the model optimizes during training.\n",
        "\n",
        "*   Key ways regularization aids XGBoost include:\n",
        "\n",
        "    * L1 Regularization (Lasso) - controlled by the alpha parameter:\n",
        "\n",
        "      Encourages sparsity by shrinking less important feature weights toward zero. This acts as feature selection and reduces model complexity.\n",
        "\n",
        "    * L2 Regularization (Ridge) - controlled by the lambda parameter:\n",
        "\n",
        "      Penalizes large weights to prevent the model from fitting noise, making it more stable and smooth.\n",
        "\n",
        "    * Minimum Child Weight:\n",
        "\n",
        "      Specifies the minimum sum of instance weights needed in a leaf. Larger values make the algorithm more conservative and reduce overfitting by preventing the creation of leaves with few samples.\n",
        "\n",
        "    * Gamma:\n",
        "\n",
        "      Sets the minimum loss reduction required to make a further split on a leaf node. A higher gamma encourages simpler trees by discouraging splits that provide small gains, thus acting as a pruning parameter.\n",
        "\n",
        "    * Subsampling (subsample) and feature subsampling (colsample_bytree):\n",
        "\n",
        "      Randomly samples training instances and features for growing trees, reducing overfitting by introducing randomness and variance.\n",
        "\n",
        "    * Early Stopping:\n",
        "\n",
        "      Halts training early if the validation error stops improving, avoiding overfitting from excessive boosting rounds.\n",
        "\n",
        "*   Thus regularization techniques enable XGBoost to build models that generalize well to unseen data, controlling overfitting, and ensuring robust predictive performance, especially useful in complex, high-dimensional datasets."
      ],
      "metadata": {
        "id": "ZEkX9_tdhdF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   CatBoost is considered highly efficient for handling categorical data due to its native and innovative approach to processing categorical features without requiring extensive manual preprocessing.\n",
        "\n",
        "*   Key reasons include:\n",
        "\n",
        "    * Native Categorical Feature Support:\n",
        "  \n",
        "      CatBoost natively supports categorical variables, eliminating the need for traditional preprocessing methods like one-hot or label encoding, which can increase dimensionality or introduce artificial ordinal relationships.\n",
        "\n",
        "    * Ordered Boosting Technique:\n",
        "\n",
        "      It uses ordered boosting that processes categorical features with permutations and statistics to reduce prediction shift and overfitting. This technique ensures that categories are encoded based on previously seen data only, avoiding target leakage.\n",
        "\n",
        "    * Efficient Encoding Strategies:\n",
        "\n",
        "      CatBoost employs advanced statistics-based encodings (like target statistics with random permutations) to convert categorical variables into numerical values while preserving valuable information and managing high cardinality features effectively.\n",
        "\n",
        "    * Combining Feature Interactions:\n",
        "\n",
        "      The algorithm automatically discovers and leverages combinations of categorical features, capturing complex interactions without exhaustive enumeration.\n",
        "\n",
        "    * Reduced Overfitting on Categorical Features:\n",
        "\n",
        "      By carefully encoding categories and applying ordered boosting, CatBoost reduces overfitting tendencies common to other boosters handling categorical data poorly.\n",
        "\n",
        "    * Improved Model Interpretability and Performance:\n",
        "\n",
        "      Handling categorical data internally streamlines the modeling pipeline, allows better use of categorical relationships, and achieves higher accuracy and robustness on datasets with mixed data types."
      ],
      "metadata": {
        "id": "kKEYjmHnhdK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Boosting techniques are preferred over bagging methods in real-world applications where the primary goal is to improve model accuracy by reducing bias, especially when working with relatively clean datasets and simpler base learners (weak learners). Some typical scenarios and applications include:\n",
        "\n",
        "    * Credit Risk and Loan Default Prediction:\n",
        "\n",
        "      Boosting methods like AdaBoost, Gradient Boosting, and XGBoost excel in capturing subtle patterns in financial data where predicting defaults accurately can minimize risks and losses.\n",
        "\n",
        "    * Customer Churn Prediction:\n",
        "\n",
        "      Boosting helps address imbalanced classes and focuses learning on difficult-to-classify customers, improving retention strategies in telecom or subscription-based services.\n",
        "\n",
        "    * Fraud Detection:\n",
        "\n",
        "      Boosting improves detection rates on complex and noisy transaction data by sequentially focusing on misclassified fraudulent cases.\n",
        "\n",
        "    * Medical Diagnosis and Disease Prediction:\n",
        "\n",
        "      The ability of boosting to incrementally reduce errors while focusing on challenging cases makes it suitable for high-stakes healthcare applications requiring high accuracy.\n",
        "\n",
        "    * Click-Through Rate (CTR) and Recommendation Systems:\n",
        "\n",
        "      Boosting models handle large-scale, sparse, and categorical-rich data effectively, improving predictions in advertising and personalized recommendations.\n",
        "\n",
        "    * Image Classification and Natural Language Processing Tasks:\n",
        "\n",
        "      Boosting ensembles, combined with feature engineering, can improve performance in classification problems involving structured data extracted from images or text.\n",
        "\n",
        "*   Overall, boosting is favored when:\n",
        "\n",
        "    * The model must achieve high accuracy with minimal bias.\n",
        "\n",
        "    * There is sufficient computational power for sequential training.\n",
        "\n",
        "    * The dataset has complex patterns suited for iterative error correction.\n",
        "\n",
        "    * Model interpretability and fine-tuning are essential for business decisions."
      ],
      "metadata": {
        "id": "OTcCQRUxhdMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Write a Python program to:\n",
        "##● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "##● Print the model accuracy\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "I5hMmywbhdR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "bc = load_breast_cancer()\n",
        "X = bc.data\n",
        "y = bc.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize AdaBoost classifier\n",
        "clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rgOnk1ssAG9",
        "outputId": "a3465d2b-73a4-466c-e9a9-eb343880841d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7:  Write a Python program to:\n",
        "##● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "##● Evaluate performance using R-squared score\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "x_Mps9ZnhdTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "\n",
        "An R-squared score of {r2:.4f} on the California Housing dataset means that the Gradient Boosting Regressor\n",
        "explains approximately {(r2 * 100):.2f}% of the variance in the target variable (median house value) on the test set.\n",
        "This is a strong indication that the model has good predictive power and fits the data well.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MH4afRosP-w",
        "outputId": "07114bdd-d236-4dba-8496-598de7e2adac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score: 0.7756\n",
            "\n",
            "\n",
            "An R-squared score of 0.7756 on the California Housing dataset means that the Gradient Boosting Regressor \n",
            "explains approximately 77.56% of the variance in the target variable (median house value) on the test set. \n",
            "This is a strong indication that the model has good predictive power and fits the data well.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Write a Python program to:\n",
        "##● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "##● Tune the learning rate using GridSearchCV\n",
        "##● Print the best parameters and accuracy.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "798ETZewhdW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "bc = load_breast_cancer()\n",
        "X = bc.data\n",
        "y = bc.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best learning rate: {grid.best_params_['learning_rate']}\")\n",
        "print(f\"Best accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy with best parameters: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh9dlpTotm9j",
        "outputId": "db7303f4-0790-47c3-c304-abd0dad5eaf3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best learning rate: 0.2\n",
            "Best accuracy: 0.9670\n",
            "Test accuracy with best parameters: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Write a Python program to:\n",
        "##● Train a CatBoost Classifier\n",
        "##● Plot the confusion matrix using seaborn\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "mORVLryVhdY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install catboost\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "bc = load_breast_cancer()\n",
        "X = bc.data\n",
        "y = bc.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_seed=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=bc.target_names, yticklabels=bc.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "True Positives (Malignant correctly predicted): 40\n",
        "\n",
        "False Negatives (Malignant misclassified as Benign): 3\n",
        "\n",
        "False Positives (Benign misclassified as Malignant): 1\n",
        "\n",
        "True Negatives (Benign correctly predicted): 70\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "5bjvvgQXuiJ0",
        "outputId": "883d42f9-46b3-4b87-be16-a2f027522ae5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "True Positives (Malignant correctly predicted): 40\n",
            "\n",
            "False Negatives (Malignant misclassified as Benign): 3\n",
            "\n",
            "False Positives (Benign misclassified as Malignant): 1\n",
            "\n",
            "True Negatives (Benign correctly predicted): 70\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "##The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "##Describe your step-by-step data science pipeline using boosting techniques:\n",
        "##● Data preprocessing & handling missing/categorical values\n",
        "##● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "##● Hyperparameter tuning strategy\n",
        "##● Evaluation metrics you'd choose and why\n",
        "##● How the business would benefit from your model\n",
        "\n",
        "Answer:\n",
        "\n",
        "Below ia a detailed step-by-step data science pipeline to predict loan default in a FinTech setting using boosting techniques with imbalanced, incomplete, and mixed-type data:\n",
        "\n",
        "1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "    Imbalanced Data:\n",
        "\n",
        "    * Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN to generate synthetic samples of the minority class (defaulters).\n",
        "\n",
        "    * Alternatively, use class weighting supported natively by most boosting algorithms, especially XGBoost and CatBoost.\n",
        "\n",
        "    Missing Values:\n",
        "\n",
        "    * For numerical columns: Impute with mean, median, or use KNN imputation as appropriate.\n",
        "\n",
        "    * For categorical columns: Impute with the most frequent category, create an “Unknown” category, or let CatBoost handle them directly as it natively supports missing values in categorical features.\n",
        "\n",
        "    Categorical Features:\n",
        "\n",
        "    * CatBoost efficiently handles categorical data natively, requiring only column indices or names to be specified.\n",
        "\n",
        "    * For XGBoost/AdaBoost, encode categorical features with One-Hot or Target Encoding—be careful with cardinality and information leakage.\n",
        "\n",
        "2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "    AdaBoost:\n",
        "\n",
        "    * Suitable for simple, clean datasets with mostly numeric features and moderate size.\n",
        "\n",
        "    XGBoost:\n",
        "\n",
        "    * Excellent general-purpose booster that handles missing values, is robust to overfitting, highly efficient, and supports custom regularization.\n",
        "\n",
        "    * Works well with moderate-scale categorical encoding, but needs manual encoding for non-numeric columns.\n",
        "\n",
        "    CatBoost:\n",
        "\n",
        "    * Strongly recommended for this use case: natively handles missing and categorical data, often requires less tuning, and typically provides superior performance and interpretability in mixed-feature datasets.\n",
        "\n",
        "    * Choose CatBoost when data is heterogeneous, has significant missing/categorical values, or if minimal preprocessing is desired.\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "    RandomizedSearchCV or GridSearchCV:\n",
        "\n",
        "    * Search over ranges for learning_rate, n_estimators, max_depth, class_weight/scale_pos_weight, subsample, and tree-specific parameters.\n",
        "\n",
        "    Early Stopping:\n",
        "\n",
        "    * Use validation AUC/ROC to halt training when improvement plateaus, reducing overfitting risk.\n",
        "\n",
        "    Cross-Validation:\n",
        "\n",
        "    * Always use stratified k-fold cross-validation due to class imbalance to maintain label distributions in all folds.\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "    Accuracy alone is not enough for imbalanced problems. Use:\n",
        "\n",
        "    * AUC-ROC: Measures overall model discriminative ability across all thresholds.\n",
        "\n",
        "    * Precision, Recall, F1-score: Especially recall for the minority (defaulter) class, as false negatives are costly.\n",
        "\n",
        "    * Confusion Matrix: For interpretable misclassification patterns and business communication.\n",
        "\n",
        "    * Precision-Recall Curve: Useful for highly skewed datasets.\n",
        "\n",
        "    * Balanced Accuracy: Adjusts for class imbalance impact.\n",
        "\n",
        "5. Business Benefits\n",
        "\n",
        "    Improved Risk Management:\n",
        "\n",
        "    * Accurately flagging likely defaulters allows proactive risk mitigation, reducing bad debts and charge-offs.\n",
        "\n",
        "    Increased Profitability:\n",
        "\n",
        "    * Enables tailored underwriting and informed approval processes, optimizing loan portfolios for profitability.\n",
        "\n",
        "    Operational Efficiency:\n",
        "\n",
        "    * Automation of large-scale loan assessments minimizes manual review burden and speeds up decision-making.\n",
        "\n",
        "    Regulatory Compliance:\n",
        "\n",
        "    * Transparent, explainable models (especially with CatBoost/XGBoost feature importance & SHAP values) support fair lending practices.\n",
        "\n",
        "    Customer Experience:\n",
        "\n",
        "    * Fast, data-driven decisions improve trust and satisfaction, while minimizing approval for high-risk applicants."
      ],
      "metadata": {
        "id": "xaMi9bXehdc2"
      }
    }
  ]
}