{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression | Assignment"
      ],
      "metadata": {
        "id": "8tzB07zTW6rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Logistic Regression is a supervised machine learning algorithm used for classification tasks, particularly binary classification (e.g., yes/no, 0/1, spam/not spam). Despite its name, it predicts categorical outcomes, not continuous ones.\n",
        "*   It works by modeling the probability that a given input belongs to a particular class. The output is a value between 0 and 1, interpreted as a probability.\n",
        "*   How does logistic regression differ from linear regression:\n",
        "\n",
        "    * Linear Regression predicts a continuous numeric value, while Logistic Regression predicts a probability between 0 and 1 representing a class label.\n",
        "\n",
        "    * Linear Regression assumes a linear relationship between input and output variables, while Logistic Regression models the log-odds (logit) of an event using a sigmoid function.\n",
        "\n",
        "    * Linear Regression uses the Least Squares Method to minimize errors, while Logistic Regression uses Maximum Likelihood Estimation (MLE) to maximize the probability of observed results.\n",
        "\n",
        "    * Linear Regression output can take any real value, while Logistic Regression output is restricted between 0 and 1.\n",
        "\n",
        "    * Linear Regression assumes a normal distribution of residuals, while Logistic Regression assumes a binomial distribution of the dependent variable.\n",
        "\n",
        "    * Linear Regression gives a straight-line fit, while Logistic Regression gives an S-shaped (sigmoid) curve.\n",
        "\n",
        "    * Linear Regression is mainly used for forecasting or trend analysis, while Logistic Regression is used for classification problems such as spam detection or disease prediction.\n",
        "\n",
        "    * In Linear Regression, evaluation metrics include R², RMSE, and MAE, while in Logistic Regression, metrics include Accuracy, Precision, Recall, and AUC."
      ],
      "metadata": {
        "id": "lI4hopxxW611"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Anwer:\n",
        "\n",
        "*   In Logistic Regression, the Sigmoid function is used to convert the output of the linear equation into a probability value between 0 and 1. It helps predict whether an observation belongs to the positive or negative class.\n",
        "\n",
        "    Sigmoid function formula:\n",
        "\n",
        "    y = 1 / (1 + e^(-z))\n",
        "\n",
        "    where\n",
        "    \n",
        "    z = b0 + b1x1 + b2x2 + ... + bn*xn\n",
        "\n",
        "    Here,\n",
        "\n",
        "    e is the base of the natural logarithm,\n",
        "\n",
        "    z is the linear combination of input variables and weights,\n",
        "\n",
        "    y represents the predicted probability (output between 0 and 1).\n",
        "\n",
        "*   Role and Importance:\n",
        "\n",
        "    * The sigmoid function transforms any real number (from -∞ to +∞) into a range between 0 and 1.\n",
        "\n",
        "      Example:\n",
        "\n",
        "      If z = 0, then y = 0.5\n",
        "\n",
        "      If z → ∞, then y → 1\n",
        "      \n",
        "      If z → -∞, then y → 0\n",
        "\n",
        "    * The curve is S-shaped, allowing Logistic Regression to handle non-linear decision boundaries smoothly.\n",
        "\n",
        "    * It enables classification by applying a threshold:\n",
        "\n",
        "      If y >= 0.5 → Class = 1\n",
        "\n",
        "      If y < 0.5 → Class = 0\n",
        "\n",
        "    * Because of its smooth gradient, it is also useful for optimization algorithms like Gradient Descent to adjust weights effectively.\n",
        "\n",
        "*   Below is the python code example demonstrating the role of the sigmoid function in Logistic Regression:"
      ],
      "metadata": {
        "id": "hlncIhXSW638"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Example values of z (linear combination output)\n",
        "z_values = np.linspace(-10, 10, 100)\n",
        "\n",
        "# Calculate sigmoid values for each z\n",
        "sigmoid_values = sigmoid(z_values)\n",
        "\n",
        "# Print some example outputs for select values of z\n",
        "print(\"Sigmoid(0) =\", sigmoid(0))      # 0.5\n",
        "print(\"Sigmoid(5) =\", sigmoid(5))      # ~0.993\n",
        "print(\"Sigmoid(-5) =\", sigmoid(-5))    # ~0.0067\n",
        "\n",
        "# Plot the sigmoid curve\n",
        "plt.plot(z_values, sigmoid_values)\n",
        "plt.title(\"Sigmoid Function Curve\")\n",
        "plt.xlabel(\"z (Linear Output)\")\n",
        "plt.ylabel(\"Sigmoid(z) (Probability)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "zH2qOiAdbA_5",
        "outputId": "69756ae5-aab0-48c9-a9ad-339a497829ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid(0) = 0.5\n",
            "Sigmoid(5) = 0.9933071490757153\n",
            "Sigmoid(-5) = 0.0066928509242848554\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Regularization in Logistic Regression is a technique to prevent overfitting by adding a penalty term to the loss function during model training.\n",
        "\n",
        "*   Why Regularization is needed:\n",
        "\n",
        "    * Logistic Regression coefficients (weights) can become very large when the model fits training data too closely (overfitting).\n",
        "\n",
        "    * Large coefficients make the model complex and reduce its ability to generalize well to new data.\n",
        "\n",
        "    * Regularization controls this by penalizing large coefficients, encouraging simpler models.\n",
        "\n",
        "*   How Regularization works:\n",
        "\n",
        "    * It modifies the loss function by adding a penalty term multiplied by a regularization parameter lambda (λ).\n",
        "\n",
        "    * The new objective to minimize is:\n",
        "      Loss + λ * R(beta)\n",
        "\n",
        "    * Common types of regularization are:\n",
        "\n",
        "      L1 regularization (Lasso):\n",
        "      R(beta) = sum(|beta_i|)\n",
        "\n",
        "      L2 regularization (Ridge):\n",
        "      R(beta) = (1/2) * sum(beta_i^2)\n",
        "\n",
        "    * The parameter λ controls how strong the penalty is:\n",
        "\n",
        "      Large λ means more penalty, shrinking coefficients toward zero (may cause underfitting)\n",
        "\n",
        "      Small λ means less penalty, allowing coefficients to grow (may risk overfitting)\n",
        "\n",
        "*   Below is the python example demonstrating Regularization in Logistic Regression using scikit-learn, showing how to apply L1 and L2 regularization:"
      ],
      "metadata": {
        "id": "GJWWa6X3W665"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with L2 regularization (default)\n",
        "model_l2 = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
        "model_l2.fit(X_train, y_train)\n",
        "pred_l2 = model_l2.predict(X_test)\n",
        "print(\"Accuracy with L2 regularization:\", accuracy_score(y_test, pred_l2))\n",
        "\n",
        "# Logistic Regression with L1 regularization\n",
        "model_l1 = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', max_iter=1000)\n",
        "model_l1.fit(X_train, y_train)\n",
        "pred_l1 = model_l1.predict(X_test)\n",
        "print(\"Accuracy with L1 regularization:\", accuracy_score(y_test, pred_l1))\n",
        "\n",
        "# Display some coefficients to see effect of regularization\n",
        "print(\"L2 coefficients:\", model_l2.coef_[0][:5])\n",
        "print(\"L1 coefficients:\", model_l1.coef_[0][:5])\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "penalty='l2' applies Ridge regularization, while penalty='l1' applies Lasso regularization.\n",
        "\n",
        "The parameter C controls regularization strength (inverse scale). Smaller C means stronger regularization.\n",
        "\n",
        "Standardizing features is important before applying regularization.\n",
        "\n",
        "The example uses breast cancer dataset for binary classification.\n",
        "\n",
        "Prints model accuracy on test data for both L1 and L2.\n",
        "\n",
        "Outputs some coefficients to show how regularization affects model weights (L1 tends to sparsify coefficients).\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1L6tUGuc2w9",
        "outputId": "b379cf21-3cc5-4b2a-8f7f-3fe813453d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with L2 regularization: 0.9824561403508771\n",
            "Accuracy with L1 regularization: 0.9824561403508771\n",
            "L2 coefficients: [-0.36619067 -0.36158121 -0.32048262 -0.41717824 -0.19220365]\n",
            "L1 coefficients: [0. 0. 0. 0. 0.]\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "penalty='l2' applies Ridge regularization, while penalty='l1' applies Lasso regularization.\n",
            "\n",
            "The parameter C controls regularization strength (inverse scale). Smaller C means stronger regularization.\n",
            "\n",
            "Standardizing features is important before applying regularization.\n",
            "\n",
            "The example uses breast cancer dataset for binary classification.\n",
            "\n",
            "Prints model accuracy on test data for both L1 and L2.\n",
            "\n",
            "Outputs some coefficients to show how regularization affects model weights (L1 tends to sparsify coefficients).\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   Common evaluation metrics for classification models are crucial because they provide insights into how well the model performs, especially under different contexts such as balanced or imbalanced datasets. These metrics help measure correctness, errors, and trade-offs between different types of misclassifications.\n",
        "\n",
        "*   Key Classification Evaluation Metrics:\n",
        "\n",
        "    Accuracy:\n",
        "    \n",
        "    * Accuracy is the ratio of the number of correct predictions (both positive and negative) to the total predictions made.\n",
        "\n",
        "    * Formula:\n",
        "    \n",
        "      Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "      \n",
        "      It is straightforward but can be misleading on imbalanced datasets (e.g., if one class is much larger than another).\n",
        "\n",
        "    * Precision: Precision tells how many of the predicted positive cases were actually positive.\n",
        "\n",
        "      Formula:\n",
        "\n",
        "      Precision = TP / (TP + FP)\n",
        "      Useful when the cost of false positives is high.\n",
        "\n",
        "    * Recall (Sensitivity):\n",
        "    \n",
        "      Recall measures how many actual positive cases the model correctly identified.\n",
        "\n",
        "      Formula:\n",
        "\n",
        "      Recall = TP / (TP + FN)\n",
        "\n",
        "      Important when missing positive cases has a high cost (false negatives).\n",
        "\n",
        "    * F1 Score:\n",
        "    \n",
        "      The harmonic mean of Precision and Recall, balancing the two metrics.\n",
        "\n",
        "      Formula:\n",
        "\n",
        "      F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "      \n",
        "      Useful when you want to balance precision and recall.\n",
        "\n",
        "    * ROC Curve and AUC (Area Under the Curve):\n",
        "    \n",
        "      The ROC curve plots the true positive rate against the false positive rate at various threshold settings. AUC summarizes the curve into a single value representing model discrimination capability.\n",
        "\n",
        "*   Why These Metrics Are Important\n",
        "\n",
        "    * Accuracy may be too optimistic for imbalanced data; for example, predicting the majority class always achieves high accuracy but poor usefulness.\n",
        "\n",
        "    * Precision helps when false positives are costly, and Recall helps when false negatives are costly.\n",
        "\n",
        "    * F1 Score provides a single measure when you need a balance between precision and recall.\n",
        "\n",
        "    * AUC-ROC gives a performance overview across all classification thresholds.\n",
        "\n",
        "    * Choosing the right metric depends on the problem context, data balance, and costs of different errors.\n",
        "\n",
        "    * This foundational understanding guides model selection, tuning, and decision-making in real-world classification tasks."
      ],
      "metadata": {
        "id": "QTymk2TKW69r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "9skyj0_rW7AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target variable to DataFrame\n",
        "\n",
        "# Split the DataFrame into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data into train and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Suppress convergence warnings during model training\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "    # Initialize and train the Logistic Regression model with max_iter=1000\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Logistic Regression model:\", accuracy)\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The breast cancer dataset is loaded from sklearn and converted into a Pandas DataFrame.\n",
        "\n",
        "The dataset is split into features (X) and target (y).\n",
        "\n",
        "Using train_test_split, 70% of the data is used for training and 30% for testing.\n",
        "\n",
        "A Logistic Regression model is trained using the training set.\n",
        "\n",
        "The model predicts labels for the test set.\n",
        "\n",
        "Accuracy is calculated comparing predicted vs actual labels.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hkhXpILnVrg",
        "outputId": "000fa3d6-2fa5-40cb-dbd6-812e8e55c0cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Logistic Regression model: 0.9707602339181286\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The breast cancer dataset is loaded from sklearn and converted into a Pandas DataFrame.\n",
            "\n",
            "The dataset is split into features (X) and target (y).\n",
            "\n",
            "Using train_test_split, 70% of the data is used for training and 30% for testing.\n",
            "\n",
            "A Logistic Regression model is trained using the training set.\n",
            "\n",
            "The model predicts labels for the test set.\n",
            "\n",
            "Accuracy is calculated comparing predicted vs actual labels.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:  Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "R3U4KW2-W7Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Suppress convergence warnings (optional)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "    # Create Logistic Regression model with L2 regularization (Ridge) and increased max_iter\n",
        "    model = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Get model coefficients\n",
        "coefficients = model.coef_\n",
        "print(\"Model Coefficients:\\n\", coefficients)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "The model uses 'penalty='l2' for Ridge regularization.\n",
        "\n",
        "C=1.0 is the regularization strength (inverse regularization parameter). Smaller values mean stronger regularization.\n",
        "\n",
        "The max_iter=1000 ensures enough iterations for convergence.\n",
        "\n",
        "After training, coefficients are printed, showing the feature weights.\n",
        "\n",
        "The accuracy on the test set helps evaluate model performance.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krCCp62uqCpZ",
        "outputId": "2c34c3fa-adf9-42ba-aff9-fc1cc080da87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            " [[ 2.14314843  0.21520964 -0.31493715  0.00892435 -0.25312017 -0.22953298\n",
            "  -0.87600322 -0.68463721 -0.33732353  0.00324068 -0.23214171  1.38063351\n",
            "   0.61838296 -0.1576031  -0.04346147  0.22679752  0.10464825 -0.07823463\n",
            "  -0.04363574  0.03083968  0.52020356 -0.44945643 -0.08307888 -0.01284422\n",
            "  -0.50476624 -0.15635753 -1.70991405 -1.07991958 -1.18680917  0.00843899]]\n",
            "Model Accuracy: 0.9707602339181286\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The model uses 'penalty='l2' for Ridge regularization.\n",
            "\n",
            "C=1.0 is the regularization strength (inverse regularization parameter). Smaller values mean stronger regularization.\n",
            "\n",
            "The max_iter=1000 ensures enough iterations for convergence.\n",
            "\n",
            "After training, coefficients are printed, showing the feature weights.\n",
            "\n",
            "The accuracy on the test set helps evaluate model performance.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "lXEBBE5DW7FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model with multi_class='ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Uses the Iris dataset which has three classes (multiclass problem).\n",
        "\n",
        "Splits dataset for training and testing.\n",
        "\n",
        "The logistic regression is set with multi_class='ovr' (one-vs-rest strategy).\n",
        "\n",
        "Uses solver='liblinear' which supports multiclass OVR efficiently.\n",
        "\n",
        "Prints the detailed classification report including precision, recall, and F1-score for each class.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ-3h2HVue4f",
        "outputId": "550e5bc8-3d0f-4c9d-fd11-6fc14efcdc74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "Uses the Iris dataset which has three classes (multiclass problem).\n",
            "\n",
            "Splits dataset for training and testing.\n",
            "\n",
            "The logistic regression is set with multi_class='ovr' (one-vs-rest strategy).\n",
            "\n",
            "Uses solver='liblinear' which supports multiclass OVR efficiently.\n",
            "\n",
            "Prints the detailed classification report including precision, recall, and F1-score for each class.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "CERESO_XW7Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, solver='liblinear'))\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'logisticregression__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'logisticregression__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Uses a Pipeline to combine StandardScaler and LogisticRegression, essential since regularization benefits from\n",
        "feature scaling.\n",
        "\n",
        "param_grid specifies values for regularization strength C and penalty norms l1 & l2.\n",
        "\n",
        "GridSearchCV exhaustively tests each parameter combination and uses 5-fold cross-validation to estimate validation\n",
        "accuracy.\n",
        "\n",
        "Outputs the set of hyperparameters yielding the best cross-validation accuracy.\n",
        "\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqVW5oVQxDl2",
        "outputId": "f6437526-44d2-479c-bd00-e3976c3e5fd6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l2'}\n",
            "Best Cross-Validation Accuracy: 0.9824406148113647\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "Uses a Pipeline to combine StandardScaler and LogisticRegression, essential since regularization benefits from \n",
            "feature scaling.\n",
            "\n",
            "param_grid specifies values for regularization strength C and penalty norms l1 & l2.\n",
            "\n",
            "GridSearchCV exhaustively tests each parameter combination and uses 5-fold cross-validation to estimate validation\n",
            "accuracy.\n",
            "\n",
            "Outputs the set of hyperparameters yielding the best cross-validation accuracy.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "KqK2efoEW7Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression WITHOUT scaling\n",
        "model_raw = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression WITH scaling\n",
        "model_scaled = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "Explanation:\n",
        "\n",
        "First model trains on original data without feature scaling.\n",
        "\n",
        "Second model trains on standardized data (zero mean, unit variance) using StandardScaler.\n",
        "\n",
        "Scaling improves gradient-based optimization and can improve accuracy and convergence.\n",
        "\n",
        "The printed accuracies help you compare impact of feature scaling on Logistic Regression performance.\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAqthdpIzAyS",
        "outputId": "2f1f2fdb-8a84-4e79-b8c6-af3e7f92eb6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9649\n",
            "Accuracy with scaling: 0.9825\n",
            "\n",
            "\n",
            "Explanation:\n",
            "\n",
            "First model trains on original data without feature scaling.\n",
            "\n",
            "Second model trains on standardized data (zero mean, unit variance) using StandardScaler.\n",
            "\n",
            "Scaling improves gradient-based optimization and can improve accuracy and convergence.\n",
            "\n",
            "The printed accuracies help you compare impact of feature scaling on Logistic Regression performance.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "Answer:\n",
        "\n",
        "*   In building a Logistic Regression model for an imbalanced e-commerce marketing campaign dataset (5% responders), a careful approach is needed:\n",
        "\n",
        "    Data Handling:\n",
        "\n",
        "    * Start with data cleaning and preprocessing.\n",
        "\n",
        "    * Separate features and the binary target (responded: yes/no).\n",
        "\n",
        "    Feature Scaling:\n",
        "\n",
        "    * Apply feature scaling (e.g., StandardScaler) as Logistic Regression benefits from standardized inputs for stable and faster convergence.\n",
        "\n",
        "    Handling Class Imbalance:\n",
        "\n",
        "    * Use class weighting in Logistic Regression (class_weight='balanced') to give more importance to minority class samples.\n",
        "\n",
        "    * Alternatively, use resampling techniques:\n",
        "\n",
        "      Oversampling the minority class (e.g., SMOTE) to synthetically generate minority class examples.\n",
        "\n",
        "      Downsampling the majority class to reduce its dominance.\n",
        "\n",
        "    * Combining resampling with class weighting can also be effective.\n",
        "\n",
        "    Hyperparameter Tuning:\n",
        "\n",
        "    * Use GridSearchCV to tune hyperparameters like regularization strength C, type of penalty (l1 or l2), and solver.\n",
        "\n",
        "    * Include tuning whether to use class weights or not (class_weight parameter).\n",
        "\n",
        "    * Use pipelines to combine scaling and model training for robust validation.\n",
        "\n",
        "    Model Evaluation:\n",
        "\n",
        "    Use evaluation metrics that reflect minority class performance such as:\n",
        "\n",
        "    * Precision, Recall, and F1-score (especially recall if missing responders is costly).\n",
        "\n",
        "    * ROC-AUC and Precision-Recall AUC, since accuracy can be misleading on imbalanced data.\n",
        "\n",
        "    Use stratified cross-validation to maintain class distribution during training and validation.\n",
        "\n",
        "\n",
        "    Example approach outline:\n",
        "\n",
        "    * Load and preprocess data.\n",
        "\n",
        "    * Split train/test set stratified by class.\n",
        "\n",
        "    * Apply standard scaling.\n",
        "\n",
        "    * Use LogisticRegression(class_weight='balanced', solver='liblinear').\n",
        "\n",
        "    * Tune hyperparameters with GridSearchCV including class_weight.\n",
        "\n",
        "    * Evaluate with AUC, precision, recall, and F1.\n",
        "\n",
        "    * Consider adjusting decision threshold to improve business-relevant trade-offs.\n",
        "\n",
        "*   This strategy helps build an unbiased, well-generalizing Logistic Regression model that pays attention to minority responders, essential for real-world marketing success.Below is the python code demonstration showing how to handle an imbalanced dataset (5% positive responses) for a marketing campaign prediction using Logistic Regression."
      ],
      "metadata": {
        "id": "odSpIIAoW7OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress convergence warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Step 1: Create an imbalanced dataset (5% positive class)\n",
        "X, y = make_classification(n_samples=5000, n_features=20, n_informative=3,\n",
        "                           n_redundant=2, n_classes=2, weights=[0.95, 0.05],\n",
        "                           random_state=42)\n",
        "\n",
        "# Step 2: Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "print(\"Original class distribution:\", np.bincount(y_train))\n",
        "\n",
        "# Step 3: Balance the dataset using SMOTE (oversampling minority class)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "print(\"Balanced class distribution:\", np.bincount(y_train_balanced))\n",
        "\n",
        "# Step 4: Standardize features for better convergence\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Define Logistic Regression and hyperparameter grid\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Step 6: Apply GridSearchCV for hyperparameter tuning\n",
        "grid = GridSearchCV(model, param_grid, scoring='roc_auc', cv=5)\n",
        "grid.fit(X_train_scaled, y_train_balanced)\n",
        "\n",
        "# Step 7: Evaluate model performance\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_prob = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid.best_params_)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rysB9NPOzbqk",
        "outputId": "5164c2c9-53cd-42ae-faee-bffd786349ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: [3308  192]\n",
            "Balanced class distribution: [3308 3308]\n",
            "\n",
            "Best Parameters: {'C': 0.1, 'class_weight': None, 'penalty': 'l2'}\n",
            "ROC-AUC Score: 0.925298427878496\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.93      1418\n",
            "           1       0.29      0.83      0.43        82\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.64      0.86      0.68      1500\n",
            "weighted avg       0.95      0.88      0.91      1500\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1255  163]\n",
            " [  14   68]]\n"
          ]
        }
      ]
    }
  ]
}